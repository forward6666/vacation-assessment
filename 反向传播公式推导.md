# 反向传播公式推导

## 神经网络结构

1. **输入层**: 包含 n 个输入节点。
2. **隐藏层**: 包含 h  个隐藏节点。
3. **输出层**: 包含  m 个输出节点。

## 数学表示

### 前向传播

1. **输入层到隐藏层**:
    - 输入向量： $ \mathbf{x} \in \mathbb{R}^n $
    - 权重矩阵：$ \mathbf{W}_1 \in \mathbb{R}^{h \times n} $ 
    - 偏置向量： $ \mathbf{b}_1 \in \mathbb{R}^h $
    - 激活函数： $ f $
    - 隐藏层输出：$ \mathbf{a}_1 = f(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1) $
2. **隐藏层到输出层**:
    - 权重矩阵： $\mathbf{W}_2 \in \mathbb{R}^{m \times h}$
    - 偏置向量： $\mathbf{b}_2 \in \mathbb{R}^m$
    - 输出层输出： $ \mathbf{y} = g(\mathbf{W}_2 \mathbf{a}_1 + \mathbf{b}_2) $
    - 其中 \( g \) 是输出层的激活函数（如 softmax 或线性函数）。

### 损失函数

设真实标签为$\mathbf{t}$，我们使用损失函数 $L(\mathbf{y}, \mathbf{t})$来衡量输出 $ \mathbf{y}$ 和真实标签  $\mathbf{t}$之间的差距。

### 反向传播

反向传播的目的是通过计算梯度来更新权重和偏置，以最小化损失函数。我们通过链式法则来计算每个参数的梯度。

1. **输出层到隐藏层**:

    - 损失函数对输出的梯度： $\frac{\partial L}{\partial \mathbf{y}}$
    - 输出对隐藏层激活的梯度： $\frac{\partial \mathbf{y}}{\partial \mathbf{a}_1} = g'(\mathbf{W}_2 \mathbf{a}_1 + \mathbf{b}_2) \cdot \mathbf{W}_2$

    所以，损失函数对隐藏层激活的梯度：
    $
    \delta_2 = \frac{\partial L}{\partial \mathbf{a}_1} = \frac{\partial L}{\partial \mathbf{y}} \cdot g'(\mathbf{W}_2 \mathbf{a}_1 + \mathbf{b}_2) \cdot \mathbf{W}_2
    $

2. **隐藏层到输入层**:

    - 损失函数对隐藏层激活的梯度： $\delta_2$ 
    - 隐藏层激活对权重的梯度： $\frac{\partial \mathbf{a}_1}{\partial \mathbf{W}_1} = f'(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1) \cdot \mathbf{x} $$

    所以，损失函数对权重的梯度：
    $
    \delta_1 = \frac{\partial L}{\partial \mathbf{W}_1} = \delta_2 \cdot f'(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1) \cdot \mathbf{x}
    $

3. **更新权重和偏置**:

    使用梯度下降法来更新权重和偏置：
    $
    \mathbf{W}_1 \leftarrow \mathbf{W}_1 - \eta \delta_1$
    
    $
    \mathbf{b}_1 \leftarrow \mathbf{b}_1 - \eta \delta_2
    $
    $
    \mathbf{W}_2 \leftarrow \mathbf{W}_2 - \eta \delta_3
    $
    
    $\mathbf{b}_2 \leftarrow \mathbf{b}_2 - \eta \delta_4
    $

    其中，$ \eta $$ 是学习率， $ $\delta_3$  和 $\delta_4 $ 是从输出层到隐藏层和隐藏层到输入层的梯度。

## 总结

以上是一个简单的两层神经网络的反向传播推导过程。通过这个过程，我们可以逐层计算梯度并更新每一层的权重和偏置，从而使得网络的损失函数逐步减少，实现网络的训练和优化。

